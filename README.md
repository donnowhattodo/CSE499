# CSE499

This is our CSE499A (Senior Design-I) + CSE499B (Senior Design-II)  Project

Our Supervisor : Dr. Mohammad Ashrafuzzaman Khan (AZK)

Group Members:

  1. Ahmed Akib Jawad Karim
  2. Anjana Tameem
  3. Kazi Hafiz Md. Asad
  
  ## Project Description
In this project, we aim to compress the BERT-base-uncased model while preserving its accuracy and attributes. BERT (Bidirectional Encoder Representations from Transformers) is a powerful language model used for various natural language processing tasks. However, the original BERT model is large and computationally expensive, limiting its usability in resource-constrained environments.

Our goal is to develop novel techniques to reduce the size of the BERT-base-uncased model without significantly sacrificing its performance. By leveraging model compression methods such as knowledge distillation, parameter pruning, and quantization, we aim to create a smaller and more efficient version of BERT that maintains its language understanding capabilities.

Throughout the project, we will experiment with different compression techniques, evaluate the trade-offs between model size and performance, and fine-tune the compressed model to achieve optimal results. The ultimate objective is to make the compressed BERT model suitable for deployment in real-world applications where resource limitations are a concern.
